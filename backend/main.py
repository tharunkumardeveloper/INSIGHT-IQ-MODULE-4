from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json
import os
from typing import Dict, List, Any
from textblob import TextBlob
import re
import uvicorn
import random
from collections import defaultdict

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Domain configurations
DOMAINS = {
    "ai_ml": {
        "name": "Artificial Intelligence & Machine Learning",
        "emoji": "\U0001F9E0",
        "competitors": ["OpenAI", "Anthropic", "DeepMind", "Hugging Face", "Stability AI"]
    },
    "cloud": {
        "name": "Cloud Computing & SaaS",
        "emoji": "\u2601\uFE0F",
        "competitors": ["AWS", "Microsoft Azure", "Google Cloud", "Salesforce", "Oracle"]
    },
    "cybersecurity": {
        "name": "Cybersecurity & Data Privacy",
        "emoji": "\U0001F510",
        "competitors": ["Palo Alto Networks", "CrowdStrike", "Fortinet", "Cloudflare", "Check Point"]
    },
    "web3": {
        "name": "Web3, Blockchain & Crypto",
        "emoji": "\U0001F310",
        "competitors": ["Coinbase", "Binance", "ConsenSys", "Chainalysis", "Polygon Labs"]
    },
    "arvr": {
        "name": "Augmented & Virtual Reality",
        "emoji": "ðŸ¥½",
        "competitors": ["Meta", "HTC Vive", "Niantic", "Magic Leap", "Varjo"]
    },
    "robotics": {
        "name": "Robotics & Automation",
        "emoji": "ðŸ¤–",
        "competitors": ["Boston Dynamics", "ABB Robotics", "iRobot", "Fanuc", "UiPath"]
    },
    "semiconductors": {
        "name": "Semiconductors & Hardware",
        "emoji": "ðŸ’¾",
        "competitors": ["Intel", "AMD", "NVIDIA", "TSMC", "Qualcomm"]
    },
    "quantum": {
        "name": "Quantum Computing",
        "emoji": "âš›ï¸",
        "competitors": ["IBM Quantum", "Rigetti", "IonQ", "D-Wave Systems", "Xanadu"]
    },
    "consumer": {
        "name": "Consumer Electronics",
        "emoji": "ðŸ“±",
        "competitors": ["Apple", "Samsung", "Sony", "LG", "Xiaomi"]
    },
    "greentech": {
        "name": "Green Tech & Energy Innovation",
        "emoji": "ðŸŒ±",
        "competitors": ["Tesla Energy", "Enphase", "Siemens Energy", "Ã˜rsted", "First Solar"]
    }
}

def compute_sentiment(text):
    """Compute sentiment score using TextBlob"""
    if pd.isna(text) or text == "":
        return 0.0
    try:
        blob = TextBlob(str(text))
        return blob.sentiment.polarity
    except:
        return 0.0

def load_and_process_csv(domain_key, csv_type):
    """Load and process CSV files with sentiment computation"""
    # Get the directory where the script is located
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)  # Go up one level from backend/
    
    # Try multiple file naming patterns
    possible_paths = [
        os.path.join(project_root, "data", f"{domain_key}_{csv_type}.csv"),
        os.path.join(project_root, "data", f"{domain_key.replace('_', '')}_{csv_type}.csv"),  # Handle ai_ml -> ai
        os.path.join(project_root, "data", f"{domain_key.replace('cybersecurity', 'security')}_{csv_type}.csv"),  # Handle security variant
        os.path.join(project_root, "data", f"{domain_key.replace('cybersecurity', 'cybersecuirty')}_{csv_type}.csv")  # Handle typo (fallback)
    ]
    
    print(f"Looking for CSV file for {domain_key}_{csv_type}")
    print(f"Script directory: {script_dir}")
    print(f"Project root: {project_root}")
    print(f"Trying paths: {possible_paths}")
    
    file_path = None
    for path in possible_paths:
        print(f"Checking path: {path} - exists: {os.path.exists(path)}")
        if os.path.exists(path):
            file_path = path
            break
    
    if not file_path:
        print(f"No CSV file found for {domain_key}_{csv_type}")
        data_dir = os.path.join(project_root, "data")
        print(f"Available files in data directory: {os.listdir(data_dir) if os.path.exists(data_dir) else 'data directory not found'}")
        return pd.DataFrame()
    
    try:
        df = pd.read_csv(file_path)
        print(f"Successfully loaded {file_path} with {len(df)} rows")
        
        # Handle different date column names
        date_columns = ['date', 'published_at', 'timestamp']
        for col in date_columns:
            if col in df.columns:
                df['date'] = pd.to_datetime(df[col], errors='coerce')
                break
        
        # Handle different text column names for sentiment analysis
        if csv_type == 'news':
            text_columns = ['summary', 'content', 'title', 'text']
            for col in text_columns:
                if col in df.columns:
                    text_col = col
                    break
            else:
                text_col = None
        else:  # social
            text_columns = ['text', 'content', 'message']
            for col in text_columns:
                if col in df.columns:
                    text_col = col
                    break
            else:
                text_col = None
        
        # Compute sentiment if missing or update existing
        if text_col and text_col in df.columns:
            if 'sentiment_score' not in df.columns:
                df['sentiment_score'] = df[text_col].apply(compute_sentiment)
            else:
                # Fill missing sentiment scores
                mask = df['sentiment_score'].isna()
                if mask.any():
                    df.loc[mask, 'sentiment_score'] = df.loc[mask, text_col].apply(compute_sentiment)
        
        # Handle different source column names
        if 'source' not in df.columns:
            if 'platform' in df.columns:
                df['source'] = df['platform']
            elif 'company' in df.columns:
                df['source'] = df['company']
            else:
                df['source'] = 'Unknown'
        
        # Ensure required columns exist
        if 'title' not in df.columns and text_col:
            df['title'] = df[text_col].str[:100] + '...'
        
        return df
    except Exception as e:
        print(f"Error loading {file_path}: {e}")
        return pd.DataFrame()

def generate_alerts(domain_key):
    """Generate realistic domain-specific alerts based on tech intelligence feed"""
    alerts = []
    current_time = datetime.now()
    
    # Comprehensive domain-specific alerts based on October 2025 tech intelligence
    domain_alerts = {
        "ai_ml": [
            {
                "type": "product_launch",
                "title": "OpenAI launches Omni-3 multimodal model",
                "description": "OpenAI introduces Omni-3 with persistent memory API for enterprise applications, enabling context retention across sessions",
                "severity": "high",
                "source": "product_monitoring",
                "hours_ago": 2
            },
            {
                "type": "innovation",
                "title": "Anthropic introduces Claude Next with on-device reasoning",
                "description": "Claude Next features on-device reasoning capabilities for privacy-focused applications, reducing cloud dependency",
                "severity": "high",
                "source": "innovation_tracker",
                "hours_ago": 6
            },
            {
                "type": "breakthrough",
                "title": "DeepMind unveils AlphaResearch platform",
                "description": "AlphaResearch enables autonomous scientific discovery, accelerating research across multiple domains",
                "severity": "high",
                "source": "research_monitor",
                "hours_ago": 12
            },
            {
                "type": "partnership",
                "title": "Hugging Face partners with Snowflake",
                "description": "Strategic partnership enables model hosting within data warehouses, improving enterprise AI integration",
                "severity": "medium",
                "source": "partnership_tracker",
                "hours_ago": 18
            },
            {
                "type": "customer_trend",
                "title": "Enterprise demand for AI model lineage tracking",
                "description": "Enterprise buyers increasingly requesting clearer AI model lineage and audit trails before adoption",
                "severity": "medium",
                "source": "customer_intelligence",
                "hours_ago": 24
            }
        ],
        "cloud": [
            {
                "type": "service_launch",
                "title": "AWS adds AI-powered cost anomaly detection",
                "description": "CloudWatch now includes AI-powered cost anomaly detection to help enterprises optimize cloud spending",
                "severity": "high",
                "source": "service_monitor",
                "hours_ago": 3
            },
            {
                "type": "product_launch",
                "title": "Azure debuts Confidential Containers",
                "description": "Microsoft Azure introduces Confidential Containers for secure AI workloads with hardware-level encryption",
                "severity": "high",
                "source": "product_monitoring",
                "hours_ago": 8
            },
            {
                "type": "integration",
                "title": "Google Cloud integrates Vertex AI Agent Builder",
                "description": "Vertex AI Agent Builder now integrated into Google Workspace for enhanced productivity applications",
                "severity": "medium",
                "source": "integration_tracker",
                "hours_ago": 14
            },
            {
                "type": "product_update",
                "title": "Salesforce rolls out Data Cloud GPT",
                "description": "Data Cloud GPT enables automated marketing workflows with generative AI capabilities",
                "severity": "medium",
                "source": "product_monitor",
                "hours_ago": 20
            },
            {
                "type": "customer_trend",
                "title": "Mid-market demand for multi-cloud visibility",
                "description": "Mid-market firms increasingly demanding multi-cloud visibility dashboards and FinOps automation tools",
                "severity": "medium",
                "source": "customer_intelligence",
                "hours_ago": 30
            }
        ],
        "cybersecurity": [
            {
                "type": "product_launch",
                "title": "Palo Alto releases Cortex XDR AI automation",
                "description": "Cortex XDR AI incident automation module reduces response times and improves threat containment",
                "severity": "high",
                "source": "product_monitoring",
                "hours_ago": 1
            },
            {
                "type": "api_launch",
                "title": "CrowdStrike launches Falcon OverWatch API",
                "description": "Real-time telemetry integration API enables custom threat hunting and incident response workflows",
                "severity": "high",
                "source": "api_tracker",
                "hours_ago": 4
            },
            {
                "type": "innovation",
                "title": "Fortinet pilots LLM-assisted firewall configuration",
                "description": "AI-powered firewall configuration reduces setup time and improves security policy accuracy",
                "severity": "medium",
                "source": "innovation_tracker",
                "hours_ago": 10
            },
            {
                "type": "open_source",
                "title": "Cloudflare open-sources ML threat-scoring model",
                "description": "Machine learning threat-scoring model now available to security community for threat detection",
                "severity": "medium",
                "source": "open_source_monitor",
                "hours_ago": 16
            },
            {
                "type": "customer_requirement",
                "title": "Unified threat intelligence demand spikes",
                "description": "Enterprise RFPs increasingly require unified threat intelligence across cloud and endpoint security",
                "severity": "high",
                "source": "customer_intelligence",
                "hours_ago": 22
            }
        ],
        "web3": [
            {
                "type": "platform_launch",
                "title": "Coinbase introduces institutional staking platform",
                "description": "New institutional staking platform includes insurance guarantees for enterprise cryptocurrency holdings",
                "severity": "high",
                "source": "platform_monitor",
                "hours_ago": 5
            },
            {
                "type": "security_innovation",
                "title": "Binance launches AI-driven fraud detection",
                "description": "AI-powered fraud-detection engine provides real-time on-chain monitoring and threat prevention",
                "severity": "high",
                "source": "security_tracker",
                "hours_ago": 9
            },
            {
                "type": "product_launch",
                "title": "ConsenSys releases MetaMask Business wallets",
                "description": "Enterprise-focused MetaMask Business wallets offer enhanced security and compliance features",
                "severity": "medium",
                "source": "product_monitoring",
                "hours_ago": 15
            },
            {
                "type": "feature_update",
                "title": "Chainalysis adds privacy-coin tracking",
                "description": "Reactor platform now includes privacy-coin tracking capabilities for enhanced compliance monitoring",
                "severity": "medium",
                "source": "feature_tracker",
                "hours_ago": 21
            },
            {
                "type": "customer_insight",
                "title": "Growing interest in proof-of-identity solutions",
                "description": "Rising Web3 regulation drives customer demand for robust proof-of-identity and compliance solutions",
                "severity": "medium",
                "source": "customer_intelligence",
                "hours_ago": 28
            }
        ],
        "arvr": [
            {
                "type": "product_launch",
                "title": "Meta unveils Quest 4 Pro with retina-resolution",
                "description": "Quest 4 Pro features retina-resolution passthrough AR for enhanced mixed reality experiences",
                "severity": "high",
                "source": "product_monitoring",
                "hours_ago": 7
            },
            {
                "type": "sdk_release",
                "title": "HTC Vive integrates hand-tracking SDK",
                "description": "Hand-tracking SDK enables industrial training applications with natural gesture controls",
                "severity": "medium",
                "source": "sdk_tracker",
                "hours_ago": 13
            },
            {
                "type": "platform_update",
                "title": "Niantic launches Lightship 2.0 AR cloud",
                "description": "Lightship 2.0 AR cloud platform provides enhanced developer tools for location-based AR experiences",
                "severity": "medium",
                "source": "platform_monitor",
                "hours_ago": 19
            },
            {
                "type": "partnership",
                "title": "Magic Leap partners with Siemens",
                "description": "Strategic partnership focuses on digital-twin visualization for industrial and manufacturing applications",
                "severity": "medium",
                "source": "partnership_tracker",
                "hours_ago": 25
            },
            {
                "type": "customer_demand",
                "title": "Surge in AR-assisted maintenance solutions",
                "description": "Manufacturing clients increasingly requesting AR-assisted maintenance and training solutions",
                "severity": "high",
                "source": "customer_intelligence",
                "hours_ago": 32
            }
        ],
        "robotics": [
            {
                "type": "product_reveal",
                "title": "Boston Dynamics reveals Atlas Next",
                "description": "Atlas Next features full-cycle manipulation AI for advanced autonomous task completion",
                "severity": "high",
                "source": "product_monitoring",
                "hours_ago": 11
            },
            {
                "type": "cloud_launch",
                "title": "ABB Robotics launches cloud orchestration",
                "description": "Cloud orchestration platform enables centralized management and coordination of collaborative robots",
                "severity": "medium",
                "source": "cloud_tracker",
                "hours_ago": 17
            },
            {
                "type": "tool_launch",
                "title": "UiPath introduces AI Workflow Studio",
                "description": "AI Workflow Studio with GenAI agents automates complex business process workflows",
                "severity": "medium",
                "source": "tool_monitor",
                "hours_ago": 23
            },
            {
                "type": "patent_filing",
                "title": "iRobot patents adaptive home navigation",
                "description": "Foundation model-based adaptive home navigation improves autonomous cleaning efficiency",
                "severity": "low",
                "source": "patent_tracker",
                "hours_ago": 29
            },
            {
                "type": "customer_voice",
                "title": "Enterprise demand for robotic telemetry integration",
                "description": "Enterprises requesting cross-system robotic telemetry integration for unified automation management",
                "severity": "medium",
                "source": "customer_intelligence",
                "hours_ago": 35
            }
        ],
        "semiconductors": [
            {
                "type": "product_ship",
                "title": "NVIDIA ships Blackwell GPUs for edge AI",
                "description": "Blackwell GPUs optimized for edge AI inferencing applications now shipping to enterprise customers",
                "severity": "high",
                "source": "product_monitoring",
                "hours_ago": 4
            },
            {
                "type": "product_release",
                "title": "AMD releases Ryzen AI 9000 with NPU",
                "description": "Ryzen AI 9000 series features integrated Neural Processing Unit for on-device AI acceleration",
                "severity": "high",
                "source": "product_tracker",
                "hours_ago": 12
            },
            {
                "type": "business_update",
                "title": "Intel spins out Foundry Services 2.0",
                "description": "Foundry Services 2.0 includes RISC-V support for custom silicon manufacturing",
                "severity": "medium",
                "source": "business_monitor",
                "hours_ago": 18
            },
            {
                "type": "manufacturing_milestone",
                "title": "TSMC achieves 1.4 nm node pilot production",
                "description": "1.4 nanometer manufacturing node reaches pilot production phase, enabling next-gen chip performance",
                "severity": "high",
                "source": "manufacturing_tracker",
                "hours_ago": 26
            },
            {
                "type": "market_signal",
                "title": "Rising demand for low-power edge AI chips",
                "description": "B2B customers increasingly requesting low-power edge chips for AI-powered camera applications",
                "severity": "medium",
                "source": "market_intelligence",
                "hours_ago": 33
            }
        ],
        "quantum": [
            {
                "type": "technical_demo",
                "title": "IBM Quantum demonstrates 500-qubit system",
                "description": "Error-mitigated 500-qubit quantum system achieves new milestone in quantum computing capability",
                "severity": "high",
                "source": "technical_monitor",
                "hours_ago": 8
            },
            {
                "type": "product_unveil",
                "title": "IonQ unveils rack-mount quantum modules",
                "description": "Rack-mount quantum modules enable data center integration for hybrid quantum-classical computing",
                "severity": "medium",
                "source": "product_monitoring",
                "hours_ago": 16
            },
            {
                "type": "open_source",
                "title": "D-Wave open-sources hybrid solver SDK",
                "description": "Hybrid quantum-classical solver SDK now available for developer community integration",
                "severity": "medium",
                "source": "open_source_monitor",
                "hours_ago": 24
            },
            {
                "type": "platform_launch",
                "title": "Rigetti launches Quantum Cloud 2.0",
                "description": "Quantum Cloud 2.0 platform provides enhanced quantum computing access and development tools",
                "severity": "medium",
                "source": "platform_tracker",
                "hours_ago": 31
            },
            {
                "type": "customer_trend",
                "title": "Early adopters seek quantum-classical APIs",
                "description": "Chemistry and finance sectors requesting quantum-classical hybrid APIs for specialized applications",
                "severity": "low",
                "source": "customer_intelligence",
                "hours_ago": 38
            }
        ],
        "consumer": [
            {
                "type": "product_preview",
                "title": "Apple previews iPhone 17 Pro AR",
                "description": "iPhone 17 Pro features built-in LIDAR 2.0 for enhanced augmented reality capabilities",
                "severity": "high",
                "source": "product_monitoring",
                "hours_ago": 6
            },
            {
                "type": "innovation_debut",
                "title": "Samsung debuts Galaxy Neural Display",
                "description": "Galaxy Neural Display uses adaptive brightness learning for personalized viewing experiences",
                "severity": "medium",
                "source": "innovation_tracker",
                "hours_ago": 14
            },
            {
                "type": "product_revival",
                "title": "Sony revives PlayStation Portable AI Edition",
                "description": "PlayStation Portable AI Edition integrates machine learning for enhanced gaming experiences",
                "severity": "medium",
                "source": "product_tracker",
                "hours_ago": 22
            },
            {
                "type": "product_announce",
                "title": "LG announces SmartHome Hub Gen 3",
                "description": "SmartHome Hub Gen 3 features multi-agent coordination for intelligent home automation",
                "severity": "low",
                "source": "product_monitoring",
                "hours_ago": 30
            },
            {
                "type": "customer_feedback",
                "title": "Growing interest in local-only AI processing",
                "description": "Consumer demand increasing for privacy-centric devices with local-only AI processing capabilities",
                "severity": "medium",
                "source": "customer_intelligence",
                "hours_ago": 36
            }
        ],
        "greentech": [
            {
                "type": "product_launch",
                "title": "Tesla Energy launches Powerwall Ultra",
                "description": "Powerwall Ultra delivers 3x storage efficiency improvement for residential energy systems",
                "severity": "high",
                "source": "product_monitoring",
                "hours_ago": 9
            },
            {
                "type": "platform_debut",
                "title": "Enphase debuts AI inverter optimization",
                "description": "AI-powered inverter optimization platform maximizes solar energy generation and grid stability",
                "severity": "medium",
                "source": "platform_tracker",
                "hours_ago": 15
            },
            {
                "type": "investment_news",
                "title": "Ã˜rsted invests in AI-forecasted wind routing",
                "description": "AI-powered offshore wind routing optimization reduces operational costs and improves efficiency",
                "severity": "medium",
                "source": "investment_monitor",
                "hours_ago": 21
            },
            {
                "type": "manufacturing_innovation",
                "title": "First Solar unveils circular manufacturing",
                "description": "Thin-film solar panels now manufactured using circular economy principles for sustainability",
                "severity": "medium",
                "source": "manufacturing_tracker",
                "hours_ago": 27
            },
            {
                "type": "customer_trend",
                "title": "Utilities seek real-time carbon tracking",
                "description": "Utility companies increasingly demanding real-time carbon tracking dashboards for ESG compliance",
                "severity": "high",
                "source": "customer_intelligence",
                "hours_ago": 34
            }
        ]
    }
    
    # Get alerts for the specific domain, fallback to ai_ml if domain not found
    domain_alert_templates = domain_alerts.get(domain_key, domain_alerts["ai_ml"])
    
    # Generate alerts with realistic timestamps
    for i, template in enumerate(domain_alert_templates):
        alert = {
            "id": f"{template['type']}_{domain_key}_{i + 1}",
            "type": template["type"],
            "title": template["title"],
            "description": template["description"],
            "severity": template["severity"],
            "timestamp": (current_time - timedelta(hours=template["hours_ago"])).isoformat(),
            "source": template["source"]
        }
        alerts.append(alert)
    
    return alerts

@app.get("/")
async def root():
    return {"message": "INSIGHTIQ API is running"}

@app.get("/api/domains")
async def get_domains():
    return JSONResponse(content=DOMAINS, media_type="application/json; charset=utf-8")

def generate_sample_data(domain_key: str, data_type: str):
    """Generate realistic sample data when CSV files are not available"""
    import random
    from datetime import datetime, timedelta
    
    domain_info = DOMAINS.get(domain_key, DOMAINS["ai_ml"])
    competitors = domain_info["competitors"]
    
    # Generate sample data with realistic volume patterns
    sample_data = []
    now = datetime.now()
    
    # Create realistic news volume patterns with some days having more activity
    total_records = 120 if data_type == "news" else 200  # More records for better distribution
    
    # Generate date weights to simulate realistic news patterns
    # Some days have more news (like weekdays, after announcements)
    date_weights = {}
    for days_ago in range(30):
        date = now - timedelta(days=days_ago)
        # Weekdays get more weight, recent days get more weight
        weekday_multiplier = 1.5 if date.weekday() < 5 else 0.7  # Mon-Fri vs Sat-Sun
        recency_multiplier = 1.0 + (30 - days_ago) / 100  # More recent = slightly more
        
        # Add some random spikes for "news events"
        spike_multiplier = random.choice([1.0, 1.0, 1.0, 1.0, 2.5, 3.0]) if random.random() < 0.15 else 1.0
        
        date_weights[days_ago] = weekday_multiplier * recency_multiplier * spike_multiplier
    
    # Normalize weights
    total_weight = sum(date_weights.values())
    for days_ago in date_weights:
        date_weights[days_ago] = date_weights[days_ago] / total_weight
    
    for i in range(total_records):
        # Choose date based on weights for more realistic distribution
        days_ago = random.choices(
            list(date_weights.keys()), 
            weights=list(date_weights.values())
        )[0]
        
        # Add some random hours/minutes to spread within the day
        hours_offset = random.randint(0, 23)
        minutes_offset = random.randint(0, 59)
        sample_date = now - timedelta(days=days_ago, hours=hours_offset, minutes=minutes_offset)
        
        # Random competitor
        company = random.choice(competitors)
        
        # Generate realistic content based on data type
        if data_type == "news":
            titles = [
                f"{company} announces new breakthrough in technology",
                f"{company} secures major funding round",
                f"{company} partners with industry leader",
                f"{company} releases innovative product update",
                f"{company} expands into new market segment",
                f"{company} reports strong quarterly earnings",
                f"{company} launches new research initiative",
                f"{company} acquires promising startup",
                f"{company} opens new development center",
                f"{company} wins major industry award"
            ]
            title = random.choice(titles)
            text = f"Latest developments from {company} show promising growth in the {domain_key.replace('_', ' ')} sector."
        else:  # social
            posts = [
                f"Excited about {company}'s latest announcement! #innovation",
                f"Great to see {company} leading the way in {domain_key.replace('_', ' ')}",
                f"{company} continues to impress with their technology",
                f"Thoughts on {company}'s new direction? Looks promising!",
                f"Following {company}'s progress closely - interesting developments",
                f"Just saw {company}'s demo - mind blown! ðŸ¤¯",
                f"{company} is definitely the future of {domain_key.replace('_', ' ')}",
                f"Congrats to {company} team on the latest milestone!",
                f"Anyone else following {company}? Thoughts?",
                f"{company}'s approach is game-changing IMO"
            ]
            title = random.choice(posts)
            text = title
        
        # Generate realistic metrics
        sentiment_score = random.uniform(-0.8, 0.8)
        sentiment_label = "positive" if sentiment_score > 0.1 else "negative" if sentiment_score < -0.1 else "neutral"
        
        record = {
            "source": data_type,
            "company": company,
            "title": title,
            "text": text,
            "url": f"https://example.com/{data_type}/{i}",
            "published_at": sample_date.isoformat(),
            "date": sample_date.isoformat(),
            "timestamp": int(sample_date.timestamp()),
            "mention_count": random.randint(5, 100),
            "sentiment_score": round(sentiment_score, 3),
            "sentiment_label": sentiment_label,
            "score": random.randint(50, 150),
            "raw_json": f'{{"domain": "{domain_key}", "engagement_metrics": {{"likes": {random.randint(10, 500)}, "shares": {random.randint(5, 200)}, "comments": {random.randint(2, 100)}}}}}'
        }
        
        # Add social-specific fields
        if data_type == "social":
            record.update({
                "likes": random.randint(10, 500),
                "shares": random.randint(5, 200),
                "comments": random.randint(2, 100),
                "retweets": random.randint(5, 200)
            })
        
        sample_data.append(record)
    
    return sample_data

@app.get("/api/data/{domain_key}/{data_type}")
async def get_data(domain_key: str, data_type: str):
    if domain_key not in DOMAINS:
        raise HTTPException(status_code=404, detail="Domain not found")
    
    if data_type not in ["news", "social"]:
        raise HTTPException(status_code=400, detail="Data type must be 'news' or 'social'")
    
    df = load_and_process_csv(domain_key, data_type)
    
    if df.empty:
        print(f"CSV file not found, generating sample data for {domain_key}_{data_type}")
        # Generate sample data when CSV is not available
        sample_data = generate_sample_data(domain_key, data_type)
        
        # Compute metrics for sample data
        sentiment_scores = [item['sentiment_score'] for item in sample_data]
        metrics = {
            "total_count": len(sample_data),
            "avg_sentiment": sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 0,
            "date_range": {
                "start": min(item['published_at'] for item in sample_data),
                "end": max(item['published_at'] for item in sample_data)
            }
        }
        
        return {"data": sample_data, "metrics": metrics}
    
    # Convert DataFrame to records
    data = df.to_dict('records')
    
    # Compute metrics
    metrics = {
        "total_count": len(df),
        "avg_sentiment": float(df['sentiment_score'].mean()) if 'sentiment_score' in df.columns else 0,
        "date_range": {
            "start": df['date'].min().isoformat() if 'date' in df.columns and not df['date'].isna().all() else None,
            "end": df['date'].max().isoformat() if 'date' in df.columns and not df['date'].isna().all() else None
        }
    }
    
    return {"data": data, "metrics": metrics}

@app.get("/api/dashboard/{domain_key}")
async def get_dashboard_data(domain_key: str):
    if domain_key not in DOMAINS:
        raise HTTPException(status_code=404, detail="Domain not found")
    
    domain_info = DOMAINS[domain_key]
    news_df = load_and_process_csv(domain_key, 'news')
    social_df = load_and_process_csv(domain_key, 'social')
    
    # Generate synthetic competitor data if no real data exists
    competitors_data = []
    for i, competitor in enumerate(domain_info["competitors"]):
        # Check multiple columns for competitor mentions
        news_mentions = 0
        social_mentions = 0
        competitor_sentiment = 0
        
        if not news_df.empty:
            # Check company column first, then title, then text
            if 'company' in news_df.columns:
                news_mentions = len(news_df[news_df['company'].str.contains(competitor, case=False, na=False)])
                competitor_news = news_df[news_df['company'].str.contains(competitor, case=False, na=False)]
            else:
                # Fallback to searching in title or text
                search_cols = ['title', 'text', 'summary', 'content']
                for col in search_cols:
                    if col in news_df.columns:
                        news_mentions = len(news_df[news_df[col].str.contains(competitor, case=False, na=False)])
                        competitor_news = news_df[news_df[col].str.contains(competitor, case=False, na=False)]
                        break
                else:
                    competitor_news = pd.DataFrame()
            
            if not competitor_news.empty and 'sentiment_score' in competitor_news.columns:
                competitor_sentiment = competitor_news['sentiment_score'].mean()
        
        if not social_df.empty:
            # Check company column first, then text
            if 'company' in social_df.columns:
                social_mentions = len(social_df[social_df['company'].str.contains(competitor, case=False, na=False)])
            else:
                search_cols = ['text', 'content', 'message']
                for col in search_cols:
                    if col in social_df.columns:
                        social_mentions = len(social_df[social_df[col].str.contains(competitor, case=False, na=False)])
                        break
        
        # If no real data found, generate some sample data for visualization
        if news_mentions == 0 and social_mentions == 0:
            # Generate realistic sample data based on competitor position
            base_mentions = max(10, len(news_df) // 10) if not news_df.empty else np.random.randint(15, 50)
            news_mentions = max(1, int(base_mentions * (0.8 - i * 0.1)))  # Decreasing by position
            social_mentions = max(1, int(base_mentions * (1.2 - i * 0.15)))
            competitor_sentiment = np.random.uniform(-0.3, 0.7)  # Realistic sentiment range
        
        competitors_data.append({
            "name": competitor,
            "news_count": news_mentions,
            "social_mentions": social_mentions,
            "sentiment": float(competitor_sentiment) if not pd.isna(competitor_sentiment) else np.random.uniform(-0.2, 0.5)
        })
    
    # Overall metrics
    combined_df = pd.concat([news_df, social_df], ignore_index=True) if not news_df.empty or not social_df.empty else pd.DataFrame()
    overall_sentiment = float(combined_df['sentiment_score'].mean()) if not combined_df.empty and 'sentiment_score' in combined_df.columns else 0.1
    
    return {
        "domain": domain_info,
        "competitors": competitors_data,
        "overall_sentiment": overall_sentiment,
        "total_news": len(news_df),
        "total_social": len(social_df)
    }

@app.get("/api/alerts/{domain_key}")
async def get_alerts(domain_key: str):
    if domain_key not in DOMAINS:
        raise HTTPException(status_code=404, detail="Domain not found")
    
    alerts = generate_alerts(domain_key)
    return {"alerts": alerts}

@app.get("/api/debug/files")
async def debug_files():
    """Debug endpoint to check file availability"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)
    data_dir = os.path.join(project_root, "data")
    
    debug_info = {
        "script_dir": script_dir,
        "project_root": project_root,
        "data_dir": data_dir,
        "data_dir_exists": os.path.exists(data_dir),
        "files": os.listdir(data_dir) if os.path.exists(data_dir) else [],
        "cwd": os.getcwd()
    }
    
    return debug_info

if __name__ == "__main__":
    print("Starting INSIGHTIQ Backend Server...")
    print("Server will be available at: http://localhost:8002")
    print("API documentation at: http://localhost:8002/docs")
    uvicorn.run(app, host="0.0.0.0", port=8002)